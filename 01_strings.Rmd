# String Theory


## Basic data types

R has several core data structures:
  
- Vectors
- Factors
- Lists
- Matrices/arrays
- Data frames


<span class="objclass">Vectors</span> form the basis of R data structures. There are two main types- <span class="objclass">atomic</span> and <span class="objclass">lists</span>. All elements of an atomic vector are the same type. 

Examples include:
  
- character
- numeric (double)
- integer
- logical

### Character strings

When dealing with text, objects of class character are what you'd typically be dealing with.  

```{r create_a_char, eval=F}
x = c('... Of Your Fake Dimension', 'Ephemeron', 'Dryswch', 'Isotasy', 'Memory')
x
```

Not much to it, but be aware there is no real limit to what is represented as a character vector. For example, in a data frame, you could have a column where each entry is one of the works of Shakespeare.

### Factors

Although not exactly precise, one can think of factors as integers with labels.  So the underlying representation of a variable for <span class="objclass">sex</span> is 1:2 with labels 'Male' and 'Female'.  They are a special class with attributes, or metadata, that contains the information about the <span class="objclass">levels</span>.

```{r factor_atts}
x = factor(rep(letters[1:3], e=10))
attributes(x)
```

While the underlying representation is numeric, it is important to remember that factors are *categorical*. They can't be used as numbers would be, as the following demonstrates.

```{r factor_sum, eval=TRUE, error=TRUE}
as.numeric(x)
sum(x)
```


Because of the integer+metadata representation, factors are actually smaller than character strings, often notably so.

```{r size_comparison}
x = sample(state.name, 10000, replace=T)
format(object.size(x), units='Kb')
format(object.size(factor(x)), units='Kb')
format(object.size(as.integer(factor(x))), units='Kb')
```

However, if memory is really a concern, it's probably not that using factors will help, but rather better hardware.


### Analysis

It is important to know that raw text cannot be analyzed quantitatively. There is no magic that takes a categorical variable with text labels and estimates correlations among words and other words or numeric data. *Everything* that can be analyzed must have some numeric representation first, and this is where factors come in. For example, here is a data frame with two categorical predictors (`factor*`), a numeric predictor (`x`), and a numeric target (`y`).  What follows is what it looks like if you wanted to run a regression model.

```{r dummy, eval=-3}
df = 
  crossing(factor_1 = c('A', 'B'),
           factor_2 = c('Q', 'X', 'J')) %>% 
  mutate(x=rnorm(6),
         y=rnorm(6))
df
model.matrix(lm(y ~ x + factor_1 + factor_2, data=df))
```
```{r dummy_pretty, echo=FALSE}
model.matrix(lm(y ~ x + factor_1 + factor_2, data=df)) %>% 
  pander()
```

The <span class="func">model.matrix</span> function exposes the underlying matrix that is actually used in the regression analysis.  You'd get a coefficient for each column of that matrix. As such, even the intercept must be represented in some fashion. For categorical data, the default coding scheme is <span class="emph">dummy coding</span>. A reference category is arbitrarily chosen (it doesn't matter which, and you can always change it), while the other categories are represented by indicator variables, where a 1 represents the corresponding label and everything else is zero.  For details on this coding scheme or others, consult any basic statistical modeling book.


### Characters vs. Factors

The main thing to note is that factors are generally a statistical phenomenon, and are required to do statistical things with data that would otherwise be a simple character string.  If you know the relatively few levels the data can take, you'll generally want to use factors, or at least know that statistical packages and methods will require them.  In addition, factors allow you to easily overcome the sometimes silly default alphabetical ordering of category levels in some very popular visualization packages.

For other things, such as text analysis, you'll almost certainly want character strings instead, and in many cases it will be required.  It's also worth noting that a lot of base R and other behavior will coerce strings to factors.  This made a lot more sense in the early days of R, but is not really necessary these days.


For more on this stuff see the following:

- http://adv-r.had.co.nz/Data-structures.html
- http://forcats.tidyverse.org/
- http://r4ds.had.co.nz/factors.html
- https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/
- http://notstatschat.tumblr.com/post/124987394001/stringsasfactors-sigh




## Basic Text Functionality

### Base R

A lot of folks new to R are not aware of just how much basic text processing R comes with out of the box.  Here are examples of note.

- <span class="func">paste</span>: glue text/numeric values together
- <span class="func">substr</span>: extract or replace substrings in a character vector
- <span class="func">grep</span> family: use regular expressions to deal with patterns of text
- <span class="func">strsplit</span>: split strings
- <span class="func">nchar</span>: how many characters in a string
- <span class="func">as.numeric</span>: convert a string to numeric if it can be
- <span class="func">adist</span>: string distances

I probably use paste/paste0 more than most things when dealing with text, as string concatenation comes up so often.

```{r paste}
paste(c('a', 'b', 'cd'), collapse='|')
paste(c('a', 'b', 'cd'), collapse='')
paste0('a', 'b', 'cd')  # shortcut to collapse=''
paste0('x', 1:3)
```

Beyond that, use of regular expression and functionality included in the <span class="func">grep</span> family is a major way to save a lot of time during data processing.  I leave that to its own section later.



### Packages

A couple packages will probably take care of the vast majority of your standard text processing needs.  Note that even if they aren't adding anything to the functionality of the base R functions, they typically will have been optimized in some fashion

- <span class="pack">stringr</span>/<span class="pack">stringi</span>: more or less the same stuff you'll find with <span class="func">substr</span>, <span class="func">grep</span> etc. except easier to use or faster
- <span class="pack">tidyr</span>: has functions such as <span class="func">unite</span>, <span class="func">separate</span>, <span class="func">replace_na</span> that can often come in handy when working with data frames
- <span class="pack">glue</span>: a newer package that can be seen as a fancier <span class="func">paste</span>. Most likely will be useful when creating functions or shiny apps in which variable text output is desired.

One issue I have with both packages and base R is that often they return a list object, when it should be simplifying to the vector format it was initially fed.  This sometimes requires an additional step or two of further processing that shouldn't be necessary, but be prepared for it[^str_all]. 

### Other

In this section I'll add some things that come to mind from time to time that might come into play when you're dealing with text.

#### Dates

Dates are not character strings. Though they may start that way, if you actually want to treat them as dates you'll need to convert the string to the appropriate date class. The <span class="pack">lubridate</span> package makes dealing with dates much easier.  It comes with conversion, extraction and other functionality that will be sure to save you some time.

```{r lubridate}
library(lubridate)
today()
today() + 1
today() + dyears(1)
leap_year(2016)
span = interval(ymd("2017-07-01"), ymd("2017-07-04"))
span
as.duration(span)
```

This package makes dates so much easier, you should always use it when dealing with them.

#### Categorical Time

In regression modeling with few time points, one often has to decide on whether to treat the year as categorical (factor) or numeric (continuous).  This greatly depends on how you want to tell your data story or other practical concerns.  For example, if you have five years in your data, treating <span class="objclass">year</span> as categorical means you are interested in accounting for unspecified things that go on in a given year.  If you treat it as numeric, you are more interested in trends. Either is fine.

#### Encoding

Encoding can be a sizable PITA sometimes, and will often come up when dealing with webscraping and other languages.  The <span class="pack">rvest</span> and <span class="pack">stringr</span> packages may be able to get you past some issues at least. See their respective functions <span class="func">repair_encoding</span> and <span class="func">str_conv</span> as starting points on this issue.


### Summary of basic text functionality

Being familiar with commonly used string functionality in base R and packages like <span class="pack">stringr</span> can save a ridiculous amount of time in your data processing.  The more familiar you are with them the easier time you'll have with text.




## Regular Expressions

A <span class="emph">regular expression</span>, regex for short, is a sequence of characters that can be used as a search pattern for a string. Common operations are to merely detect, extract, or replace the matching string.  There are actually many different flavors of regex for different programming languages, which are all flavors that originate with the Perl approach, or can enable the Perl approach to be used.  However, knowing one means you pretty much know the others with only minor modifications if any.

To be clear, not only is regex another language, it's nigh on indecipherable.  You will not learn much regex, but what you do learn will save a potentially enormous amount of time you'd otherwise spend trying to do things in a more haphazard fashion. Furthermore, practically every situation that will come up has already been asked and answered on [Stack Overflow](https://stackoverflow.com/questions/tagged/regex), so you'll almost always be able to search for what you need.

Here is an example:

`^r.*shiny[0-9]$`

What is *that* you may ask?  Well here is an example of what it would and wouldn't match.

```{r regex_intro_ex}
grepl(c('r is the shiny', 'r is the shiny1', 'r shines brightly'), pattern='^r.*shiny[0-9]$')
```

What the regex esoterically is attempting to match is any string that starts with 'r' and ends with 'shiny_' where _ is some single digit.  Specifically it breaks down as follows:

- **^** : starts with, so ^r means starts with r
- **.** : any character
- **\*** : match the preceding zero or more times
- **shiny** : match 'shiny'
- **[0-9]** : any digit 0-9 (note that we are still talking about strings, not actual numbered values)
- **$** : ends with preceding


### Typical Uses

None of it makes sense, so don't attempt to do so. Just try to remember a couple key approaches, and search the web for the rest.

Along with ^ . * [0-9], a couple more common ones are:

- **[a-z]** : letters a-z
- **[A-Z]** : capital letters
- **+** : match the preceding one or more times
- **()** : groupings
- **|** : logical or e.g. [a-z]|[0-9]  (a lower case letter or a number)
- **?** : preceding item is optional, and will be matched at most once. Typically used for 'look ahead' and 'look behind'
- **\\** : escape a character, like if you actually wanted to search for a period, you'd use \\., though in R you need \\\\, i.e. double slashes, for escape.

In addition, there are certain predefined characters that can be called:

- **[:punct:]** : punctuation
- **[:blank:]** : spaces and tabs
- **[:alnum:]** : alphanumeric characters

Those are just a few.  The key functions can be found by looking at the help file for the <span class="func">grep</span> function (`?grep`).  However, the <span class="pack">stringr</span> package has the same functionality with perhaps a slightly faster processing (though that's due to the underlying <span class="pack">stringi</span> package).

See if you can guess which of the following will turn up true.

```{r eval=FALSE}
grepl(c('apple', 'pear', 'banana'), pattern='a')
grepl(c('apple', 'pear', 'banana'), pattern='^a')
grepl(c('apple', 'pear', 'banana'), pattern='^a|a$')
```


Scraping the web, munging data, just finding things in your scripts ... you can potentially use this all the time, and not only with text analysis, as we'll now see.

### dplyr helper functions

The <span class="pack">dplyr</span> package comes with some poorly documented[^poordoc] but quite useful helper functions that essentially serve as human-readable regex, which is a very good thing.  These functions allow you to select variables[^helperrows] based on their names.  They are just calling <span class="func">grep</span> in the end.

- <span class="func">starts_with</span>: starts with a prefix (same as regex '^blah')
- <span class="func">ends_with</span>: ends with a prefix     (same as regex 'blah$')
- <span class="func">contains</span>: contains a literal string  (same as regex 'blah')
- <span class="func">matches</span>: matches a regular expression (put your regex here)
- <span class="func">num_range</span>: a numerical range like x01, x02, x03.  (same as regex 'x[0-9][0-9]')
- <span class="func">one_of</span>: variables in character vector. (if you need to quote variable names, e.g. within a function)
- <span class="func">everything</span>: all variables.  (a good way to spend time doing something only to accomplish what you would have by doing nothing, or a way to reorder variables)

## Examples

### Example 1

Let's say you're dealing with some data that has been handled typically, that is to say, poorly. For example, you have a variable in your data representing whether something is from the north or south region.

```{r label_problem, echo=FALSE}
df = data_frame(
  id = 1:500,
  x = round(rnorm(500), 2), 
  region = sample(c('north', 'north ', 'south', 'South', ' South', 'North ', 'North'), 500, replace=T)
)
DT::datatable(df, 
              rownames=F,
              options=list(dom='t', 
                           scrollX='50%',
                           autoWidth = TRUE,
                           columnDefs = list(list(width = '50px', targets = 0),
                                             list(className = 'dt-center', targets = 1)))
)
```


It might seem okay until...

```{r label_problem2, echo=1, eval=2}
table(df$region)
pander(table(df$region))
```

Even if you spotted the casing issue, there is still a white space problem[^excel]. Let's say you want this to be capitalized 'North' and 'South'. How might you do it? It's actually quite easy with the <span class="pack">stringr</span> tools.

```{r label_problem3, eval=FALSE}
library(stringr)
df %>% 
  mutate(region = str_trim(region),
         region = str_to_title(region))
```

The <span class="func">str_trim</span> function trims white space from either side, while <span class="func">str_to_title</span> converts everything to first letter capitalized.  

```{r label_problem4, echo=2, eval=1:2}
df_corrected = df %>% 
  mutate(region = str_trim(region),
         region = str_to_title(region))
table(df_corrected$region)
pander(table(df_corrected$region))
```

### Example 2

Suppose you import a data frame, and the data was originally in wide format, where each column represented a year of data collection for the individual. Since it is bad form for data columns to have numbers for names, when you import it, the result looks like the following.

```{r rename_chunk, echo=FALSE}
df = data.frame(id=1:20, round(matrix(rnorm(100), ncol=5), 2))
DT::datatable(df, rownames=F,
              options=list(dom='tp',
                           autoWidth = TRUE,
                           columnDefs = list(list(width = '50px', targets = 0),
                                             list(className = 'dt-center', targets = 1)))
              )
```

<br>

So the problem now is to change the names to be Year_1, Year_2, etc. You might think you might have to use <span class="func">colnames</span> and manually create a string of names to replace the current ones.


```{r rename_chunk2, eval=FALSE}
colnames(df)[-1] = c('Year_1', 'Year_2', 'Year_3', 'Year_4', 'Year_5')
```

Or perhaps you're thinking of the paste0 function, which works fine and saves some typing.

```{r rename_chunk3, eval=FALSE}
colnames(df)[-1] = paste0('Year_', 1:5)
```

However, data sets may be hundreds of columns, and the columns of data may have the same pattern but not be next to one another.  For example, the first few dozen columns are all data that belongs to the first wave, etc. It is tedious to figure out which columns you don't want, but even then you're resulting to using magic numbers with the above approach, and one column change to data will mean that redoing the name change will fail.

However, the following accomplishes what we want, and is reproducible regardless of where the columns are in the data set.


```{r rename_chunk4}
df %>% 
  rename_at(vars(num_range('X', 1:5)), 
            str_replace, pattern='X', replacement='Year_') %>% 
  head
```

We just have to use the <span class="func">num_range</span> helper function within the function that tells <span class="func">rename_at</span> what it should be renaming, and let <span class="func">str_replace</span> do the rest. 



## Exercises

[^poordoc]: At least they're exposed now.

[^excel]: This is a very common issue among Excel users, and just one of the many reasons not to use it.

[^helperrows]: And maybe some day, the rows. For now you'll have to use a <span class="func">grepl</span>/<span class="func">str_detect</span> approach.

[^str_all]: I also don't think it necessary to have separate functions for str_* functions in <span class="pack">stringr</span> depending on whether, e.g. I want 'all' matches (practically every situation) or just the first (very rarely). It could have just been an additional argument with default `all=TRUE`.