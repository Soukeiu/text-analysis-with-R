# Topic modeling


## Basic idea

<span class="emph">Topic modeling</span> as typically done is a tool for much more than text.  The primary technique of <span class="emph">latent dirichlet allocation</span> should be as much a part of your toolbox as principal components and factor analysis.  It can be seen merely as a dimension reduction approach, but it can also be used for its rich interpretative quality as well. The basic idea is that we'll take a whole lot of terms, loosely defined, and boil them down to a few topics.   In this sense LDA is akin to discrete PCA.  Another way to think about this is more from the  perspective of factor analysis, where we are keenly interested in interpretation of the result, and want to know both what terms are associated with which topics, and what documents are more likely to present which topics.  

In the standard setting, to be able to conduct such an analysis from text one needs a <span class="emph">document-term matrix</span>, where rows represent documents, and columns terms. Each cell is a count of how many times the term occurs in the document. Terms are typically words, but could be any <span class="emph">n-gram</span> of interest. Outside of text analysis they could represent bacteria, genetic information, or who knows what. 


## Examples


```{r eval=F}
rnj_filtered %>% 
   count(word) %>% 
   arrange(desc(n)) %>% data.frame
```

```{r rnj_remove_names, eval=F}
rnj_names = rnj %>% 
  slice(10:35) %>% 
  mutate(name=str_extract(text, pattern='.+?(?=,)'),
         name_lower = str_to_lower(name)) 
rnj_names

```

```{r eval=F}
oldE = data_frame(word = c('thou', 'thee', 'thine', 'thy',
                           'ye', 'hath', 'art', 'tis',
                           'hast', 'ay', 'ere', 'dost'))
rnj_filtered2 = rnj_filtered %>% 
  filter(!word %in% pull(rnj_names, name_lower)) %>% 
  anti_join(stop_words) %>% 
  anti_join(oldE)

rnj_filtered2 %>% 
   count(word) %>% 
   arrange(desc(n))
```

```{r test_tm_map, eval=FALSE}
test = rnj %>% 
  slice(-(1:49)) %>% 
  filter(!text == str_to_upper(text),
         !str_detect(text, pattern='^Scene|^Act |^\\[')) %>% 
  select(-gutenberg_id) %>% 
  unnest_tokens(para, input=text, token='paragraphs') %>% 
  mutate(paraID = 1:n())

skipWords <- function(x) removeWords(x, c("it", "the", pull(oldE, word)))
funs <- list(stripWhitespace,
             skipWords,
             removePunctuation,
             content_transformer(tolower),
             stemDocument)
test_tm = tm_map(VCorpus(DataframeSource(select(test, para))), FUN = tm_reduce, tmFuns = funs)[[1]]
content(test_tm)

```

```{r test_quanteda, eval=FALSE}
library(quanteda)
test = rnj %>% 
  slice(-(1:49)) %>% 
  filter(!text == str_to_upper(text),
         !str_detect(text, pattern='^Scene|^Act |^\\[')) %>% 
  select(-gutenberg_id) %>% 
  unnest_tokens(text, input=text, token='paragraphs', to_lower=F) %>% 
  mutate(paraID = 1:n())
testq = corpus(test)
summary(testq)
```
