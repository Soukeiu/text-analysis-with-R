# Shakespeare Start to Finish

The following attempts to demonstrate the usual difficulties in dealing with text by procuring and processing the works of Shakespeare.  The source is [MIT](http://shakespeare.mit.edu/) which has made the 'complete' works available on the web since 1993, plus one other from Gutenberg.  The initial issue is simply getting the works from the web.  Subsequently there is metadata, character names, stopwords etc. to be removed. At that point we can stem and count the words in each work, which, when complete, puts us at the point we are ready for analysis.

The primary packages used are <span class="pack">tidytext</span>, <span class="pack">stringr</span>, and when things are ready for analysis, <span class="pack">quanteda.</span>



## ACT I. Scrape Moby and Gutenberg Shakespeare

### Scene I. Scrape main works

Initially we must scrape the web to get the documents we need.  The <span class="pack">rvest</span> package will be used as follows.

- Start with the url of the site
- Get the links off that page to serve as base urls for the works
- Scrape the document for each url
- Deal with the collection of Sonnets separately
- Write out results


```{r shakes_urls, eval=FALSE}
library(rvest); library(tidyverse); library(stringr)

page0 = read_html('http://shakespeare.mit.edu/')

works_urls0 = page0 %>%
  html_nodes('a') %>%
  html_attr('href')

main = works_urls0 %>%
  grep(pattern='index', value=T) %>%
  str_replace_all(pattern='index', replacement='full')

other = works_urls0[!grepl(works_urls0, pattern='index|edu|org|news')]

works_urls = c(main, other)
works_urls[1:3]
```

Now we just paste the main site url to the work urls and download them.  Here is where we come across our first snag.  The <span class="func">html_text</span> function has what I would call a bug but what the author feels is a feature.  [Basically it ignores line breaks of the form <br> in certain situations](https://github.com/hadley/rvest/issues/175).  This means it will smash text together that shouldn't be, thereby making *any* analysis of it fairly useless[^bug].  Luckily, [\@rentrop](https://github.com/rentrop) provided a solution, which is in `r/fix_read_html.R`.

```{r scrape_shakes, eval=FALSE}
works0 = lapply(works_urls, function(x) read_html(paste0('http://shakespeare.mit.edu/', x)))

source('r/fix_read_html.R')

html_text_collapse(works0[[1]]) #works
works = lapply(works0, html_text_collapse)


names(works) = c("All's Well That Ends Well" "As You Like It" "Comedy of Errors"
                 "Cymbeline" "Love's Labour's Lost" "Measure for Measure"
                 "The Merry Wives of Windsor" "The Merchant of Venice" "A Midsummer Night's Dream"
                 "Much Ado about Nothing" "Pericles Prince of Tyre" "The Taming of the Shrew"
                 "The Tempest" "Troilus and Cressida" "Twelfth Night"
                 "The Two Gentlemen of Verona" "The Winter's Tale" "King Henry IV Part 1"
                 "King Henry IV Part 2" "Henry V" "Henry VI Part 1"
                 "Henry VI Part 2" "Henry VI Part 3" "Henry VIII"
                 "King John" "Richard II" "Richard III"
                 "Antony and Cleopatra" "Coriolanus" "Hamlet"
                 "Julius Caesar" "King Lear" "Macbeth"
                 "Othello" "Romeo and Juliet" "Timon of Athens"
                 "Titus Andronicus" "Sonnets" "A Lover's Complaint"
                 "The Rape of Lucrece" "Venus and Adonis" "Elegy")
```


### Scene II. Sonnets 

We hit a slight nuisance with the Sonnets. The Sonnets have a bit of a different structure than the plays. All links are in a single page, with a different form for the url, and each sonnet has its own page.

```{r scrape_sonnets, eval=F}
sonnet_urls = paste0('http://shakespeare.mit.edu/', grep(works_urls0, pattern='sonnet', value=T)) %>%
  read_html() %>%
  html_nodes('a') %>%
  html_attr('href')

sonnet_urls = grep(sonnet_urls, pattern = 'sonnet', value=T)  # remove amazon link

# read the texts
sonnet0 = purrr::map(sonnet_urls, function(x) read_html(paste0('http://shakespeare.mit.edu/Poetry/', x)))

# collapse to one 'Sonnets' work
sonnet = sapply(sonnet0, html_text_collapse)
works$Sonnets = sonnet
```


### Scene III. Save and write out 

Now we can save our results so we won't have to repeat any of the previous scraping.  We want to save the main text object as an RData file, and write out the texts to their own file.  When dealing with text, you'll regularly want to save stages so you can avoid repeating what you don't have to, as often you will need to go back after discovering new issues further down the line.

```{r initial_save, eval=F}
save(works, file='data/texts_raw/shakes/moby_from_web.RData')

# This will spit the text to the console unfortunately

purrr::map2(works,
            paste0('data/texts_raw/shakes/moby/', str_replace_all(names(works), " |'", '_'), '.txt'),
            function(x, nam) write_lines(x, path=nam))
```

### Scene IV. Read text from files

After the above is done, it's not required to redo, so we can always get what we need.  I'll start with the raw text as files, as that is one of the more common ways one deals with documents.  When text is nice and clean, this can be fairly straightforward.

The  function at the end comes from the <span class="pack">tidyr</span> package. Up to that line, each element in the <span class="objclass">text</span> column is the entire text, while the column itself is thus a 'list-column'. In other words we have a 42 x 2 matrix. But to do what we need, we'll want to have access to each line, and the <span class="func">unnest</span> function unpacks each line within the title. The first few lines of the result is shown after.

```{r read_shakes_works, echo=-5, eval=FALSE}
library(tidyverse); library(stringr)

shakes0 =
  data_frame(file = dir('data/texts_raw/shakes/moby/', full.names = TRUE)) %>%
  mutate(text = map(file, read_lines)) %>%
  transmute(id = basename(file), text) %>%
  unnest(text)

save(shakes0, file='data/initial_shakes_dt.RData')

# Alternate that provides for more options
# library(readtext)
# shakes0 =
#   data_frame(file = dir('data/texts_raw/shakes/moby/', full.names = TRUE)) %>%
#   mutate(text = map(file, readtext, encoding='UTF8')) %>%
#   unnest(text)
```

```{r show_shakes0, echo=F}
load('data/initial_shakes_dt.RData')
DT::datatable(shakes0[1:20,], options=list(dom='tp'))
```



### Scene V. Add additional works 

It is typical to be gathering texts from multiple sources. In this case, we'll get *The Phoenix and the Turtle* from the Gutenberger website.  There is an R package that will allow us to work directly with the site, making the process straightforward[^r4everything].  I also considered two other works, but I refrained from "The Two Noble Kinsmen" because like many other of Shakespeare's versions on Gutenberg, it's basically written in a different language.  I also refrained from  *The Passionate Pilgrim* because it's mostly not Shakespeare.

When first doing this project, I actually started with Gutenberg, but it became a notable PITA. The texts were inconsistent in source, and sometimes reproduced printing errors purposely, which would have compounded typical problems.  I thought it could have been solved by using the *Complete Works of Shakespeare* but the download only came with that title, meaning one would have to hunt for and delineate each separate work.  This might not have been too big of an issue, except that there is no table of contents, nor consistent naming of titles across different printings.  The MIT approach, on the other hand, was a few lines of code. This represents a common issue in text analysis when dealing with sources, a different option may save a lot of time in the end.

The following code could be more succinct to deal with one text, but I initially was dealing with multiple works, so I've left it in that mode.  In the end we'll have a <span class="objclass">tibble</span> with an id column for the file/work name, and another column that contains the lines of text.

```{r phoenix_turtle, eval=FALSE}
library(gutenbergr)

works_not_included = c("The Phoenix and the Turtle")   # add others if desired

gute0 = gutenberg_works(title %in% works_not_included)
gute = lapply(gute0$gutenberg_id, gutenberg_download)
gute = mapply(function(x, y) mutate(x, id=y) %>% select(-gutenberg_id), 
              gute, works_not_included, 
              SIMPLIFY=F)

shakes = shakes0 %>%
  bind_rows(gute) %>%
  mutate(id = str_replace_all(id, " |'", '_')) %>% 
  mutate(id = str_replace(id, '.txt', '')) %>% 
  arrange(id)

# shakes %>% split(.$id) # inspect

save(shakes, file='data/texts_raw/shakes/shakes_df.RData')
```

## ACT II. Preliminary Cleaning

If you think we're even remotely getting close to being ready for analysis, I say Ha! to you. Our journey has only just begun (cue the Carpenters). 

Now we can start thinking about prepping the data for eventual analysis. One of the nice things about having the data in a tidy format is that we can use string functionality over the column of text in a simple fashion.

### Scene I. Remove initial text/metadata

First on our todo list is to get rid of all the prelimnary text of titles, authorship etc.  This is fairly easy when you think that every text will start with ACT I, or in the case of the Sonnets, the word 'Sonnet'.  We want to drop all text up to those points.  I've created a [function](https://github.com/m-clark/text-analysis-with-R/blob/master/r/detect_first_act.R) that will do that, and then just apply it to each works tibble[^tibblefail].  For the poems and *A Funeral Elegy for Master William Peter*, we look instead for the line where his name or initials start the line.

```{r remove_preliminary_text, echo=-(1:2)}
load('data/texts_raw/shakes/shakes_df.RData')

source('r/detect_first_act.R')

shakes_trim = shakes %>%
  split(.$id) %>%
  lapply(detect_first_act) %>%
  bind_rows

shakes %>% filter(id=='Romeo_and_Juliet') %>% head
shakes_trim %>% filter(id=='Romeo_and_Juliet') %>% head
```

### Scene II. Miscellaneous removal

Next, we'll want to remove empty rows, any remaining titles, lines that denote the act or scene, and other stuff.  I'm going to remove the word *prologue* and *epilogue* as a stopword later. While some texts have a line that just says that (`PROLOGUE`), others have text that describes the scene (`Prologue. Blah blah`) and which I've decided to keep. As such, we just need the word itself gone.

```{r remove_misc}
titles =  c("A Lover's Complaint", "All's Well That Ends Well", "As You Like It", "The Comedy of Errors",
            "Cymbeline", "Love's Labour's Lost", "Measure for Measure",
            "The Merry Wives of Windsor", "The Merchant of Venice", "A Midsummer Night's Dream",
            "Much Ado about Nothing", "Pericles Prince of Tyre", "The Taming of the Shrew",
            "The Tempest", "Troilus and Cressida", "Twelfth Night",
            "The Two Gentlemen of Verona", "The Winter's Tale", "King Henry IV, Part 1",
            "King Henry IV, Part 2", "Henry V", "Henry VI, Part 1",
            "Henry VI, Part 2", "Henry VI, Part 3", "Henry VIII",
            "King John", "Richard II", "Richard III",
            "Antony and Cleopatra", "Coriolanus", "Hamlet",
            "Julius Caesar", "King Lear", "Macbeth",
            "Othello", "Romeo and Juliet", "Timon of Athens",
            "Titus Andronicus", "Sonnets", 
            "The Rape of Lucrece", "Venus and Adonis", "A Funeral Elegy", "The Phoenix and the Turtle")

shakes_trim = shakes_trim %>%
  filter(text != '',                     # empties
         !text %in% titles,              # titles
         !str_detect(text, '^ACT|^SCENE|^Enter|^Exit|^Exeunt|^Sonnet')  # acts etc.
         )

shakes_trim %>% filter(id=='Romeo_and_Juliet') # we'll get prologue later
```

### Scene III. Classification of works

While we're at it, we can save the classical (sometimes arbitrary) classifications of Shakespeare's works for later comparison to what we'll get in our analyses. We'll save them to call as needed.

```{r shakespeare_classes, eval=FALSE}
shakes_types = data_frame(title=unique(shakes_trim$id)) %>%
  mutate(class = 'Comedy',
         class = if_else(grepl(title, pattern='Adonis|Lucrece|Complaint|Turtle|Pilgrim|Sonnet|Elegy'), 'Poem', class),
         class = if_else(grepl(title, pattern='Henry|Richard|John'), 'History', class),
         class = if_else(grepl(title, pattern='Troilus|Coriolanus|Titus|Romeo|Timon|Julius|Macbeth|Hamlet|Othello|Antony|Cymbeline|Lear'), 'Tragedy', class),
         problem = if_else(grepl(title, pattern='Measure|Merchant|^All|Troilus|Timon|Passion'), 'Problem', 'Not'),
         late_romance = if_else(grepl(title, pattern='Cymbeline|Kinsmen|Pericles|Winter|Tempest'), 'Late', 'Other'))

save(shakes_types, file='data/shakespeare_classification.RData') # save for later
```

## ACT III. Stop words

As we've noted before, we'll want to get rid of stop words, things like articles, possessive pronouns, and other very common words.  In this case, we also want to include character names.  However, the big wrinkle here is that this is not everyday English, so we need to get ye, thee, thine etc.  In addition there are things that need to be replaced like o'er to over, which may then also be removed. In short, this is not so straightforward.

### Scene I. Character names

We'll get the list of character names from opensourceshakespeare.org via <span class="pack">rvest</span>, but I added some from the poems and others that came through, e.g. abbreviated names.

```{r character_names, eval=FALSE, echo=-1}
library(rvest)
shakes_char_url = 'https://www.opensourceshakespeare.org/views/plays/characters/chardisplay.php'
page0 = read_html(shakes_char_url)
tabs = page0 %>% html_table()
shakes_char = tabs[[2]][-(1:2), c(1,3,5)] # remove header and phantom columns
colnames(shakes_char) = c('Nspeeches', 'Character', 'Play')
shakes_char = shakes_char %>%
  distinct(Character,.keep_all=T)

save(shakes_char, file='data/shakespeare_characters.RData')
```

A new snag is that some characters with multiple names may be represented (typically) by the first or last or in the case of three, the middle, e.g. Sir Toby Belch. Others are still difficultly named e.g. RICHARD PLANTAGENET (DUKE OF GLOUCESTER). The following should capture everything by splitting the names on spaces, removing parentheses, and keeping unique terms.

```{r character_names_clean, echo=-1}
load('data/shakespeare_characters.RData')

# remove paren and split
chars = shakes_char$Character
chars = str_replace_all(chars, '\\(|\\)', '')
chars = str_split(chars, ' ') %>%
  unlist

# these were found after intial processsing
chars_other = c('enobarbus', 'marcius', 'katharina', 'clarence','pyramus',
                'andrew', 'arcite', 'perithous', 'hippolita', 'schoolmaster',
                'cressid', 'diomed', 'kate', 'titinius', 'Palamon', 'Tarquin',
                'Lucrece', 'isidore', 'tom')

chars = unique(c(chars, chars_other))

chars =  chars[chars != '']
sample(chars)[1:3]
```

### Scene II. Old, Middle, & Modern English

While Shakespeare is considered [Early Modern English](https://en.wikipedia.org/wiki/Early_Modern_English), some text may be more historical, so I include Middle and Old English stopwords, as they were readily available from the <span class="pack">cltk</span> Python module ([link](https://github.com/cltk/cltk)) and also found.  I also added some things to the ME list like "thou'ldst" that I found lingering after initial passes. In my initial use of Gutenberg texts, the Old English might have had some utility, but with these texts it only removes 'wit', so I refrain from using it.

```{r old_middle_modern_english}
# old and me from python cltk module; 
# em from http://earlymodernconversions.com/wp-content/uploads/2013/12/stopwords.txt; 
# I also added some to me

old_stops0 = read_lines('data/old_english_stop_words.txt')
# sort(old_stops0)
old_stops = data_frame(word=str_conv(old_stops0, 'UTF8'),
                      lexicon = 'cltk')

me_stops0 = read_lines('data/middle_english_stop_words')
# sort(me_stops0)
me_stops = data_frame(word=str_conv(me_stops0, 'UTF8'),
                      lexicon = 'cltk')

em_stops0 = read_lines('data/early_modern_english_stop_words.txt')
# sort(em_stops0)
em_stops = data_frame(word=str_conv(em_stops0, 'UTF8'),
                      lexicon = 'emc')
```

### Scene III. Remove stopwords

We're now ready to start removing words. However, right now, we have lines not words.  We can use the <span class="pack">tidytext</span> function <span class="func">unnnest_tokens</span>, which is like <span class="func">unnnest</span> from <span class="pack">tidyr</span>, but works on different tokens, e.g. words, sentences, or paragraphs. Note that by default, the function will make all words lower case.

```{r unnest_words}
library(tidytext)

shakes_words = shakes_trim %>%
  unnest_tokens(word, text, token='words')
```

We also will be doing a little stemming here. I'm getting rid of suffixes that end with the suffix after an apostrophe.  Many of the remaining words will either be stopwords or need to be further stemmed later. I also created a middle/modern English stemmer for words that are not caught otherwise (<span class="func">me_st_stem</span>).  Again this is the sort of thing you discover after initial passes (e.g. 'criedst'). We can do that, then the <span class="func">anti_join</span> of all the stopwords.

```{r anti_join}
source('r/st_stem.R')

shakes_words = shakes_words %>%
  mutate(word = str_trim(word),    # remove possible whitespace
         word = str_replace(word, "'er$|'d$|'t$|'ld$|'rt$|'st$|'dst$", ''),    # remove me style endings
         word = vapply(word, me_st_stem, 'a')) %>%
  anti_join(em_stops) %>%
  anti_join(me_stops) %>%
  anti_join(data_frame(word=str_to_lower(c(chars, 'prologue', 'epilogue')))) %>%
  anti_join(data_frame(word=str_to_lower(paste0(chars, "'s")))) %>%     # remove possessive names
  anti_join(stop_words)
```

As before, you should do a couple spot checks.

```{r stopword_check, results='hold'}
any(shakes_words$word == 'romeo')
any(shakes_words$word == 'prologue')
any(shakes_words$word == 'mayst')
```

## ACT IV. Other fixes

Now we're ready to finally do the word counts.  Just kidding! There is *still* work to do the for the remainder, and you'll continue to spot things after runs.  A big issue is the wrods that end in 'st' and 'est', and others that are not consistently spelled or otherwise need to be dealt with.  For example, 'crost' will not be stemmed to 'cross', as 'crossed' would be.  Finally, I limit the result to any words that have more than two characters, as my inspection dis


```{r other_fixes}
# porter should catch remaining 'est'

add_a =  c('mongst', 'gainst')  # words to add a to

shakes_words = shakes_words %>%
  mutate(word = if_else(word=='honour', 'honor', word),
         word = if_else(word=='durst', 'dare', word),
         word = if_else(word=='wast', 'was', word),
         word = if_else(word=='dust', 'does', word),
         word = if_else(word=='curst', 'cursed', word),
         word = if_else(word=='blest', 'blessed', word),
         word = if_else(word=='crost', 'crossed', word),
         word = if_else(word=='accurst', 'accursed', word),
         word = if_else(word %in% add_a,
                        paste0('a', word),
                        word),
         word = str_replace(word, "'s$", ''),                # strip remaining possessives
         word = if_else(str_detect(word, pattern="o'er"),    # change o'er over
                        str_replace(word, "'", 'v'),
                        word)) %>%
  filter(!(id=='Antony_and_Cleopatra' & word == 'mark')) %>%     # mark here is almost exclusively the character name
  filter(str_count(word)>2)
```

At this point we could still maybe add things to this list of additional fixes, but I think it's time to actually start playing with the data.


## ACT V. Fun stuff

We are finally ready to get to the fun stuff.  Finally! And now things get easy. 

### Scene I. Count the terms

We can get term counts with standard <span class="pack">dplyr</span> approaches, and <span class="pack">tidytext</span> will take that and do the other things we might want. Specifically, we can use the latter to create the document term matrix which will be used in other analysis.  The function <span class="func">cast_dfm</span> will create a dfm class object, or 'document-feature' matrix, which is the same thing but recognizes this sort of stuff is not specific to words.  With word counts in hand, now would be a good save point since they'll serve as the basis for other things.

```{r term_counts, results='hold', eval=-8}
term_counts = shakes_words %>%
  group_by(id, word) %>%
  count

term_counts %>% 
  arrange(desc(n))

library(quanteda)
shakes_dtm = term_counts %>%
  cast_dfm(document=id, term=word, value=n)

save(shakes_words, term_counts, shakes_dtm, file='data/shakes_words_df.RData')
```

Now things are looking like Shakespeare, with love for everyone[^love].  You'll notice I've kept place names such as Rome, but this might be something you'd prefer to remove.  Other candidates would be madam, woman, man, majesty (as in 'his/her') etc.  This sort of thing is up to the researcher.


### Scene II. Stemming

Now we'll stem the words.  This will make words like eye and eyes just *ey*, or war, wars and warring to *war*. In other words it will reduce variations of a word to a common root form, or 'word stem'.  We could have done this in a step prior to counting the terms, but then you only have the stemmed result to work with for the document term matrix from then on.  Depending on your situation, you may or may not want to stem, or maybe you'd want to compare results.  The <span class="pack">quanteda</span> package will actually stem with the DTM and collapse the word counts accordingly.   I note the difference in words before and after.

```{r stem, echo=-(1:4), results='hold'}
load('data/shakes_words_df.RData')

library(quanteda)

shakes_dtm
ncol(shakes_dtm)

shakes_dtm = shakes_dtm %>%
  dfm_wordstem()

shakes_dtm
ncol(shakes_dtm)
```

The result is notably fewer columns, which will speed up any analysis, as well as a slightly more dense matrix.

### Scene III. Exploration

```{r top_20_init, echo=F}
top20 = topfeatures(shakes_dtm, 20)
```

#### Top features

Let's start looking more in depth.  The following shows the 20 most common words and their respective counts. This is an easy way to find candidates to add to the stopword list.  Note that dai and prai are stems for day and pray. Love occurs `r round(top20[1]/top20[2], 2)` times as much as the most frequent word!

```{r top_20}
top20 = topfeatures(shakes_dtm, 20)
top20
```

The following is a wordcloud. They are among the most useless visual displays imaginable.  Just because you can, doesn't mean you should.

```{r wordcloud, echo=FALSE}
# useless!
textplot_wordcloud(shakes_dtm, min.freq = 400, random.order = T,
                   rot.per = .25,
                   colors = viridis::viridis(30))
```

If you want to display relative frequency do so.

```{r better_cloud, echo=FALSE}
library(forcats)
data_frame(term = names(top20), freq = top20) %>%
  mutate(
    term = fct_reorder(term, freq, .desc = TRUE),
    percentage = freq/ncol(shakes_dtm)
    ) %>% 
  ggplot() +
  geom_text(aes(x=term, y=percentage, size=percentage, label=term), 
            color='#ff5500',
            show.legend = F) +
  lazerhawk::theme_trueMinimal() +
  theme(axis.text.x = element_blank())
```

#### Similarity

The <span class="pack">quanteda</span> package has some built in similarity measures such as [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity), which you can treat similarly to the standard correlation (also available).  I display it visually to better get a sense of things.

```{r similarity, echo=1, eval=2:3}
textstat_simil(shakes_dtm, margin = "documents", method = "cosine") 

textstat_simil(shakes_dtm, margin = "documents", method = "cosine") %>%
  heatR::corrheat(show_grid = F, height = 500)

textstat_simil(dfm_weight(shakes_dtm, 'relFreq'), margin = "documents", method = "correlation") %>%
  heatR::corrheat()
```

<br>

We can already begin to see the clusters of documents.  For example, the more historical are the clump in the upper left.  The oddball is [*The Phoenix and the Turtle*](https://en.wikipedia.org/wiki/The_Phoenix_and_the_Turtle), though *Lover's Complaint* and the *Elegy* are also less similar than standard Shakespeare.  The Phoenix and the Turtle is about the death of ideal love, represented by the Phoenix and Turtledove, for which there is a funeral.  It actually is considered by scholars to be in stark constrast to his other output. Elegy itself is actually written for a funeral. [*A Lover's Complaint*](https://en.wikipedia.org/wiki/A_Lover%27s_Complaint) is considered an inferior work by the Bard, so perhaps what we're seeing is a reflection of that lack of quality.  In geneal, we're seeing things that we might expect.

#### Readability

We can examine readablity scores for the texts, but for this we'll need them in raw form. Unfortunately the concept of <span class="emph">readabiltiy</span> is ill-defined, and as such, there are dozens of measures available.  The following is based on the Coleman-Liau grade score (higher grade = more difficult).

```{r readability}
raw_texts = data_frame(file = dir('data/texts_raw/shakes/moby/', full.names = TRUE)) %>%
         mutate(text = map(file, read_lines)) %>% 
  bind_rows(data_frame(file = dir('data/texts_raw/shakes/gutenberg/', full.names = TRUE)) %>%
              mutate(text = map(file, read_lines))) %>% 
  mutate(file = str_extract(file, '[^/]*$')) %>% 
  rename(id=file)
raw_texts
raw_text_corpus = corpus(tm::VCorpus(tm::VectorSource(raw_texts$text)))
shakes_read = textstat_readability(raw_text_corpus) 

shakes_read %>% 
  mutate(id=raw_texts$id) %>% 
  select(Coleman.Liau.grade) %>% 
  heatmaply::heatmaply(Rowv=NA, Colv=NA, labRow=raw_texts$id, colors='Blues')
# shakes_read %>% 
#   mutate(id=raw_texts$id) %>% 
#   gather(key=index, value=score, -id) %>% 
#   group_by(index) %>% 
#   mutate(s_score=scales::rescale(score)) %>% 
#   ungroup %>% 
#   select(-score) %>% 
#   spread(key = index, value=s_score) %>% 
#   heatmaply::heatmaply(Rowv=NA, Colv=NA)
```

#### Lexical diversity

There are also metrics of <span class="emph">lexical diversity</span>.  As with readibility, there is no one way to measure 'diversity'.  Here we'll go back to using the standard DTM.

```{r lexical_diversity}
scale(textstat_lexdiv(shakes_dtm) ) %>% 
  heatmaply::heatmaply(Rowv=F, 
                       Colv=F, 
                       colors='RdBu',            # note that RCB scales will not work unless method='plotly'
                       plot_method = 'plotly',
                       labRow=shakes_10@documents)
```


### Scene IV. Topic model

I'd say we're now ready for topic model.  We'll run run one with 10 topics.  We can also see how things compare with the usual classifications for the texts.  Also, this will take a while to run depending on your machine (maybe a minute or two).

```{r topic_model, echo=1:2, eval=F}
library(topicmodels)
shakes_10 = LDA(convert(shakes_dtm, to = "topicmodels"), k = 10)
save(shakes_10, file='data/shakespeare_topic_model.RData')
```

```{r tm10_results, eval=FALSE}
get_terms(shakes_10, 20)
t(topics(shakes_10, 3))
```

```{r tm10_results_pretty_terms, echo=FALSE}
load('data/shakespeare_topic_model.RData')
library(topicmodels)
get_terms(shakes_10, 20) %>% DT::datatable(options=list(dom='pt'))
```

<br>

```{r tm10_results_pretty_topic_classification, echo=FALSE}
t(topics(shakes_10, 3)) %>% 
  data.frame %>% 
  rename_all(str_replace, 'X', 'Topic ') %>% 
  DT::datatable(options=list(dom='pt'))
```

<br>

The following visualization shows a heatmap for the topic probabilities of each document.  Darker values mean higher probability for a document expressing that topic.  I've also added a cluster analysis based on the previous cosine (dis)similarity matrix, and the resulting dendrogram.  


```{r viz_topics, echo=FALSE, cache=FALSE, fig.height=6}
load('data/shakespeare_classification.RData')

suppressPackageStartupMessages(library(dendextend))
row_dend  = (1-quanteda::textstat_simil(shakes_dtm, margin = "documents", method = "cosine")) %>%
  as.dist() %>%
  hclust(method="complete") %>%
  as.dendrogram %>%
  set("branches_k_color", k = 3) %>% set("branches_lwd", c(.5,.5)) %>%
  ladderize

# heatmaply is barely beta use d3heatmap for now
shakes_10@gamma %>%
  round(3) %>%
  heatmaply::heatmaply(Rowv=row_dend,
                       Colv=F,
                       labRow=shakes_10@documents,
                       labCol=paste0('topic', 1:10),
                       colors='Oranges',
                       k_row = 4,
                       plot_method = 'plotly',
                       row_side_colors = select(arrange(shakes_types, title), class),
                       fontsize_row=7,
                       fontsize_col=7,
                       colorbar_len = 0.2) %>%
  layout(width=900, height=900, showlegend=F) %>% # showing the legend will screw up the colorbar and any associated options
  lazerhawk::theme_plotly()
shakes_10@gamma %>%
  # round(3) %>%
  d3heatmap::d3heatmap(Rowv=row_dend,
                       Colv=F,
                       labRow=shakes_10@documents,
                       labCol=paste0('topic', 1:10),
                       # colors=viridis::inferno(500),
                       colors='Blues',
                       k_row = 10,
                       xaxis_font_size = 12,
                       yaxis_font_size=8, 
                       colorbar_len = 0.2,
                       show_grid = F, 
                       width=800,
                       height=800) #%>% 
  # layout(width=900, height=900, showlegend=F) %>% # showing the legend will screw up the colorbar and any associated options
  # lazerhawk::theme_plotly()
```


<br>

A couple things stand out.  Traditional classificiation really probably only works for the historical works.  Furthermore, tragedies and comedies might hit on the same topics, albeit from different perspecitves.  In addition, at least some works are very poetical, or at least have topics in common with the poems.

The following shows the average topic probability for each of the traditional classes. Topics are represented by their first five most probable terms.

```{r avg_topic_probs_per_class, echo=FALSE}
class_probs = shakes_10@gamma %>%
  data.frame() %>% 
  round(3) %>%
  rename_all(function(x) stringr::str_replace_all(x, 'X', 'Topic ')) %>% 
  bind_cols(doc=shakes_10@documents, arrange(shakes_types, title), .) %>% 
  select(-title) %>% 
  gather(key=topic, value=prob, -doc, -class, -problem, -late_romance) %>% 
  mutate(topic = forcats::fct_reorder(topic, rep(1:10, e=n_distinct(doc)))) %>% 
  group_by(class, topic) %>% 
  summarise(mean_prob = mean(prob)) %>% 
  arrange(class, topic) %>% 
  spread(topic, mean_prob) %>% 
  ungroup

class_probs %>% 
  select(-class) %>% 
  d3heatmap::d3heatmap(Rowv=F,
                       Colv=F,
                       labRow=class_probs$class,
                       labCol=apply(get_terms(shakes_10, 5), 2, paste0, collapse=' '),
                       # colors=viridis::inferno(500),
                       colors='Oranges',
                       k_row = 10,
                       # row_side_colors = select(arrange(shakes_types, title), class),
                       xaxis_font_size = 8,
                       yaxis_font_size=12, 
                       show_grid = F, 
                       width=800,
                       xaxis_height = 150
                       ) 
class_probs %>% 
  select(-class) %>% 
  heatmaply::heatmaply(Rowv=F,
                       Colv=F,
                       labRow=class_probs$class,
                       labCol=apply(get_terms(shakes_10, 10), 2, paste0, collapse='\n'),
                       # colors=viridis::inferno(500),
                       colors='Oranges',
                       plot_method = 'plotly',
                       xaxis_font_size = 8,
                       yaxis_font_size=12, 
                       xaxis_height = 250,
                       colorbar_len=.5,
                       column_text_angle=0
                       ) 
```


```{r ggplay, echo=F, eval=F}
shakes_10@gamma %>%
  data.frame() %>% 
  round(3) %>%
  rename_all(function(x) stringr::str_replace_all(x, 'X', 'Topic_')) %>% 
  bind_cols(doc=shakes_10@documents, .) %>% 
  gather(key=topic, value=prob, -doc) %>% 
  mutate(topic = forcats::fct_reorder(topic, rep(1:10, e=n_distinct(doc)))) %>% 
  ggplot(aes(x=prob, y=doc, height=..density..)) +
  geom_joy(scale=20, fill='#ff5500', alpha=.25, color='#ff5500') +
  scale_y_discrete(expand=c(0.01, 0)) +
  scale_x_continuous(expand=c(0, 0)) + 
  lazerhawk::theme_trueMinimal()
```

```{r ggplay2, echo=F, eval=F}
shakes_10@gamma %>%
  data.frame() %>% 
  round(3) %>%
  rename_all(function(x) stringr::str_replace_all(x, 'X', 'Topic_')) %>% 
  bind_cols(doc=shakes_10@documents, .) %>% 
  gather(key=topic, value=prob, -doc) %>% 
  mutate(topic = forcats::fct_reorder(topic, rep(1:10, e=n_distinct(doc)))) %>% 
  ggplot(aes(x=prob, y=topic, height=..density..)) +
  geom_joy(scale=4, fill='#ff5500', alpha=.25, color='#ff5500') +
  scale_y_discrete(expand=c(0.01, 0)) +
  scale_x_continuous(expand=c(0, 0)) + 
  lazerhawk::theme_trueMinimal()
```


[^bug]: If you can think of a use case where `x<br>y<br>z` leading to `xyz` would be both expected as default behavior and desired please let me know.

[^r4everything]: If this surprises you, let me remind you that there are over 10k packages on CRAN alone.

[^tibblefail]: I found it easier to work with the entire data frame for the function, hence splitting it on id and recombining.  Some attempt was made to work within the tidyverse, but there were numerous issues to what should have been a fairly easy task.

[^love]: Love might as well be a stopword for Shakespere.