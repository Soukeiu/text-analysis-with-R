---
title: "In the beginning was the word.."
subtitle: 'An Introduction to Text Processing and Analysis with R'
author:  |
  <div class="title"><span style="font-size:125%; font-variant:small-caps; ">Michael Clark</span><br><br>
  <span class="" style="font-size:75%">http://m-clark.github.io/workshops/</span><br><br>
  <img src="img/signature-acronym.png" style="width:30%; padding:10px 0;"> <br>
  <img src="img/ARC-acronym-signature.png" style="width:21%; padding:10px 0;"> </div>
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    css: [css/standard_html.css, css/book.css]
    hightlight: pygments
    number_sections: false
    # split_by: section
    config:
      toc:
        collapse: chapter
        scroll_highlight: yes
        before: null
        after: null
      toc_depth: 2
      toolbar:
        position: fixed
      edit : null
      download: null
      search: yes
      # fontsettings:
      #   theme: white
      #   family: sans
      #   size: 2
      sharing:
        facebook: yes
        twitter: yes
        google: no
        weibo: no
        instapper: no
        vk: no
        all: ['facebook', 'google', 'twitter', 'weibo', 'instapaper']
always_allow_html: yes
font-import: http://fonts.googleapis.com/css?family=Roboto|Open+Sans
font-family: 'Roboto'
documentclass: book
# bibliography: refs.bib
biblio-style: apalike
link-citations: yes
description: "An Introduction to  Text Analysis with R"
cover-image: img/nineteeneightyR.png
url: 'https\://m-clark.github.io/Workshops/'  # evidently the \: is required or you'll get text in the title/toc area
github-repo:  m-clark/
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=F, warning=F, error=F, comment=NA, R.options=list(width=120),   # code 
                      dev.args=list(bg = 'transparent'), dev='svg',                             # viz
                      cache.rebuild=F, cache=T)                                                 # cache
```

```{r packages, include=FALSE, cache=FALSE}
library(tidyverse); library(stringr); library(pander); library(plotly); library(lazerhawk)
```


# 

<!--chapter:end:index.Rmd-->

# Introduction

This work is in progress. Until this sentence is deleted you should probably ignore everything in it.

## Overview

Dealing with text is typically not even considered in the applied statistical training of most disciplines.  This is in direct conflict with how often it has to be dealt with prior to analysis, or how interesting it might be to have text be the focus of analysis.  This document and corresponding workshop will aim to provide a sense of the things one can do with text, and the sorts of analyses that might be useful.  It must be stressed that this is only a starting point.

### Goals

The goal of this workshop is primarily to provide a sense of common tasks related to dealing with text as part of the data or the focus of analysis, and provide some relatively easy to use tools.  Additionally, we'll have exercises to practice, but those comfortable enough to do so should follow along with the in-text examples.  Note that there is more content here than will be covered in a 2 hour workshop.


### Prerequisites

The document is for the most part very applied in nature, and doesn't assume much beyond familiarity with the R statistical computing environment.

Note the following color coding used in this document:

- <span class="emph">emphasis</span>
- <span class="pack">package</span>
- <span class="func">function</span>
- <span class="objclass">object/class</span>
- [link]()


## Initial Steps

0. Download the zip file at . Be mindful of where you put it.
1. Unzip it. Be mindful of where you put the resulting folder.
2. Open RStudio.
3. File/Open Project and click on the blue icon in the folder you just created.
4. File/Open Click on the ReadMe file and do what it says.


<!--chapter:end:00_intro.Rmd-->

# String Theory


## Basic data types

R has several core data structures:
  
- Vectors
- Factors
- Lists
- Matrices/arrays
- Data frames


<span class="objclass">Vectors</span> form the basis of R data structures. There are two main types- <span class="objclass">atomic</span> and <span class="objclass">lists</span>. All elements of an atomic vector are the same type. 

Examples include:
  
- character
- numeric (double)
- integer
- logical

### Character strings

When dealing with text, objects of class character are what you'd typically be dealing with.  

```{r create_a_char, eval=F}
x = c('... Of Your Fake Dimension', 'Ephemeron', 'Dryswch', 'Isotasy', 'Memory')
x
```

Not much to it, but be aware there is no real limit to what is represented as a character vector. For example, in a data frame, you could have a column where each entry is one of the works of Shakespeare.

### Factors

Although not exactly precise, one can think of factors as integers with labels.  So the underlying representation of a variable for <span class="objclass">sex</span> is 1:2 with labels 'Male' and 'Female'.  They are a special class with attributes, or metadata, that contains the information about the <span class="objclass">levels</span>.

```{r factor_atts}
x = factor(rep(letters[1:3], e=10))
attributes(x)
```

While the underlying representation is numeric, but it is important to remember that factors are *categorical*. They can't be used as numbers would be, as the following demonstrates.

```{r factor_sum, eval=TRUE, error=TRUE}
as.numeric(x)
sum(x)
```


Because of the integer+metadata representation, factors are actually smaller than character strings, often notably so.

```{r size_comparison}
x = sample(state.name, 10000, replace=T)
format(object.size(x), units='Kb')
format(object.size(factor(x)), units='Kb')
format(object.size(as.integer(factor(x))), units='Kb')
```

However, if memory is really a concern, it's probably not that using factors will help, but rather better hardware.


### Analysis

It is important to know that raw text cannot be analyzed quantitatively. There is no magic that takes a categorical variable with text labels and estimates correlations among words and other words or numeric data. *Everything* that can be analyzed must have some numeric representation first, and this is where factors come in. For example, here is a data frame with two categorical predictors (`factor*`), a numeric predictor (`x`), and a numeric target (`y`).  What follows is what it looks like if you wanted to run a regression model.

```{r dummy, eval=-3}
df = 
  crossing(factor_1 = c('A', 'B'),
           factor_2 = c('Q', 'X', 'J')) %>% 
  mutate(x=rnorm(6),
         y=rnorm(6))
df
model.matrix(lm(y ~ x + factor_1 + factor_2, data=df))
```
```{r dummy_pretty, echo=FALSE}
model.matrix(lm(y ~ x + factor_1 + factor_2, data=df)) %>% 
  pander()
```

The <span class="func">model.matrix</span> function exposes the underlying matrix that is used in the regression.  You'd get a coefficient for each column of that matrix. As such, even the intercept must be represented in some fashion. For categorical data, the default coding scheme is <span class="emph">dummy coding</span>. A reference category is arbitrarily chosen (it doesn't matter which), while the other categories are represented by indicator variables, where a 1 represents the corresponding label and everything else is zero.  For details on this coding scheme or others, consult any basic statistical modeling book.


### Characters vs. Factors

The main thing to note is that factors are generally a statistical phenomenon, and are required to do statistical things with data that would otherwise be a simple character string.  If you know the relatively few levels the data can take, you'll generally want to use factors, or at least know that statistical packages and methods will require them.  In addition, factors allow you to easily overcome the sometimes silly default alphabetical ordering of category levels in some very popular visualization packages.

For other things, such as text analysis, you'll almost certainly want character strings instead, and in many cases it will be required.  It's also worth noting that a lot of base R and other behavior will coerce strings to factors.  This made a lot more sense in the early days of R, but not so much these days.


For more on this stuff see the following:

- http://adv-r.had.co.nz/Data-structures.html
- http://forcats.tidyverse.org/
- http://r4ds.had.co.nz/factors.html
- https://simplystatistics.org/2015/07/24/stringsasfactors-an-unauthorized-biography/
- http://notstatschat.tumblr.com/post/124987394001/stringsasfactors-sigh




## Basic Text Functionality

### Base R

A lot of folks new to R are not aware of just how much basic text processing R comes with out of the box.  Here are examples of note.

- <span class="func">paste</span>: glue text/numeric values together
- <span class="func">substr</span>: extract or replace substrings in a character vector
- <span class="func">grep</span> family: use regular expressions to deal with patterns of text
- <span class="func">strsplit</span>: split strings
- <span class="func">nchar</span>: how many characters in a string
- <span class="func">as.numeric</span>: convert a string to numeric if it can be
- <span class="func">adist</span>: string distances

I probably use paste/paste0 more than most things when dealing with text, as string concatenation comes up so often.

```{r paste}
paste(c('a', 'b', 'cd'), collapse='|')
paste(c('a', 'b', 'cd'), collapse='')
paste0('a', 'b', 'cd')  # shortcut to collapse=''
paste0('x', 1:3)
```

Beyond that, use of regular expression and functionality included in the <span class="func">grep</span> family is a major way to save a lot of time during data processing.  I leave that to its own section later.



### Packages

A couple packages will probably take care of the vast majority of your standard text processing needs.  Note that even if they aren't adding anything to the functionality of the base R functions, they typically will have been optimized in some fashion

- <span class="pack">stringr</span>/<span class="pack">stringi</span>: more or less the same stuff you'll find with <span class="func">substr</span>, <span class="func">grep</span> etc. except easier to use or faster
- <span class="pack">tidyr</span>: has functions such as <span class="func">unite</span>, <span class="func">separate</span>, <span class="func">replace_na</span> that can often come in handy when working with data frames
- <span class="pack">glue</span>: a newer package that can be seen as a fancier <span class="func">paste</span>. Most likely will be useful when creating functions or shiny apps in which variable text output is desired.

One issue I have with both packages and base R is that often they return a list object, when it should be simplifying to the vector format it was initially fed.  This sometimes requires an additional step or two of further processing that shouldn't be necessary, but be prepared for it[^str_all]. 

### Other

#### Dates

Dates are not character strings. Though they may start that way, if you actually want to treat them as dates you'll need to convert the string to the appropiate date class. The <span class="pack">lubridate</span> package makes dealing with dates much easier.  It comes with conversion, extraction and other functionality that will be sure to save you some time.

```{r lubridate}
library(lubridate)
today()
today() + 1
today() + dyears(1)
leap_year(2016)
span = interval(ymd("2017-07-01"), ymd("2017-07-04"))
span
as.duration(span)
```

#### Categorical Time

In regression modeling with few time points, one often has to decide on whether to treat the year as categorical (factor) or numeric (continuous).  This greatly depends on how you want to tell your data story or other practical concerns.  For example, if you have five years in your data, treating <span class="objclass">year</span> as categorical means you are interested in accounting for unspecified things that go on in a given year.  If you treat it as numeric, you are more interested in trends. Either is fine.



### Summary of basic text functionality

Being familiar with commonly used string functionality in base R and packages like <span class="pack">stringr</span> can save a ridiculous amount of time in your data processing.




## Regular Expressions


## dplyr helper functions

The <span class="pack">dplyr</span> package comes with some poorly documented[^poordoc] helper functions that essentially serve as human-readable regex, which is a good thing.  These functions allow you to select variables based on their names.  They are just calling <span class="func">grep</span> in the end.

- <span class="func">starts_with</span>: starts with a prefix (same as regex '^blah')
- <span class="func">ends_with</span>: ends with a prefix     (same as regex 'blah$')
- <span class="func">contains</span>: contains a literal string  (same as regex 'blah')
- <span class="func">matches</span>: matches a regular expression (put your regex here)
- <span class="func">num_range</span>: a numerical range like x01, x02, x03.  (same as regex 'x[0-9][0-9]')
- <span class="func">one_of</span>: variables in character vector. (if you need to quote variable names, e.g. within a function)
- <span class="func">everything</span>: all variables.  (a good way to spend time doing something only to accomplish what you would have by doing nothing, or a way to reorder variables)

## Examples

### Example 1

Let's say you're dealing with some data that has been handled typically, that is to say, poorly. For example, you have a variable in your data representing whether something if from the north or south.

```{r label_problem, echo=FALSE}
df = data_frame(
  id = 1:500,
  x = round(rnorm(500), 2), 
  region = sample(c('north', 'north ', 'south', 'South', ' South', 'North ', 'North'), 500, replace=T)
)
DT::datatable(df)
```


It might seem okay until...

```{r label_problem2}
table(df$region)
```

Even if you spotted the casing issue, there is still a white space problem[^excel]. Let's say you want this to be capitalized 'North' and 'South'. How might you do it? It's actually quite easy with the <span class="pack">stringr</span> tools.

```{r label_problem3}
library(stringr)
df %>% 
  mutate(region = str_trim(region),
         region = str_to_title(region))
```

The <span class="func">str_trim</span> function trims white space from either side, while <span class="func">str_to_title</span> converts everything to first letter capitalized.  

### Example 2

Suppose you import a data frame, and the data was originally in wide format, where each column represented a year of data collection for the individual. Since it is bad form for data columns to have numbers for names, when you import it, the result looks like the following

```{r rename_chunk}
df = data.frame(id=1:20, round(matrix(rnorm(100), ncol=5), 2))
DT::datatable(df, options=list(dom='pt'), rownames=F)
```

<br>

So the problem now is to change the names to be Year_1, Year_2, etc. You might think you might have to use colnames and manually create a string of names to replace the current ones.


```{r rename_chunk2, eval=FALSE}
colnames(df)[-1] = c('Year_1', 'Year_2', 'Year_3', 'Year_4', 'Year_5')
```

Or perhaps you're thinking of the paste0 function, which works fine and saves some typing.

```{r rename_chunk3, eval=FALSE}
colnames(df)[-1] = paste0('Year_', 1:5)
```

However, data sets may be hundreds of columns, and the columns of data may have the same pattern but not be next to one another.  For example, the first few dozen columns are all data that belongs to the first wave, etc. It is tedious to figure out which columns you don't want, but even then you're resulting to using magic numbers with the above approach, and one column change to data will mean that redoing the name change will fail.

However, the following accomplishes what we want, and is reproducible regardless of where the columns are in the data set.


```{r rename_chunk4}
df %>% 
  rename_at(vars(num_range('X', 1:5)), 
            str_replace, pattern='X', replacement='Year_') %>% 
  head
```

We just have to use the <span class="func">num_range</span> helper function within and let <span class="func">str_replace</span> do the rest. 



## Exercises

[^poordoc]: At least they're exposed now.

[^excel]: This is a very common issue among Excel users, and just one of the many reasons not to use it.

[^str_all]: I also don't think it necessary to have separate functions for str_* functions in <span class="pack">stringr</span> depending on whether, e.g. I want 'all' matches (practically every situation) or just the first (hasn't come up). It could have just been an additional argument.

<!--chapter:end:01_strings.Rmd-->

# Sentiment Analysis is Sick Yo


## Basic idea

A common and intuitive approach to text is <span class="emph">sentiment analysis</span>.  In a grand sense we are interested in the emotional content of some text, e.g. posts on Facebook, tweets, or movie reviews.  Most of the time, this is obvious when one reads it, but if you have hundreds of thousands or millions of strings to analyze, we'd like to be able to do so efficiently.

We will use the <span class="pack">tidytext</span> package for our demonstration.  It comes with a lexicon of positive and negative words that is actually a combination of multiple sources, one of which provides numeric ratings.


```{r lexicon, echo=-1}
set.seed(1234)
library(tidytext)
sentiments %>% slice(sample(1:nrow(sentiments)))
```

The gist is that we are dealing with a specific, pre-defined vocabulary.  Of course, any analysis will only be as good as the lexicon. The goal is usually to assign a sentiment score to a text, possibly an overall score or specifically positive or negative. Given that, other analyses may be implemented to predict sentiment via standard regression or machine learning.

## Issues

### Context, sarcasm, etc.

Now consider the following.

```{r sent_is_sick}
sentiments %>% filter(word=='sick') 
```

The word *sick* has been used at least since 1960s surfing culture to be slang for positive.  A basic approach to sentiment analysis as described here will not be able to detect slang or other context like sarcasm.  

### Lexicons

In addition, the lexicons are going to maybe be applicable to *general* usage of English in the western world.  Some might wonder where exactly these came from or who decided that the word *abacus* should be affiliated with 'trust'. You may start your path by typing `?sentiments` at teh console if you have the <span class="pack">tidytext</span> package loaded.

## Examples

### 

### The first thing the baby did wrong.

```{r baby_sentiment_importraw, echo=FALSE}
library(tidytext)
barth0 = 
  data_frame(file = dir('data/texts_raw/barthelme/', full.names = TRUE)) %>%
  mutate(text = map(file, read_lines)) %>%
  transmute(id = basename(file), text) %>%
  unnest(text) 

# Fix encoding, convert to sentences
barth = barth0 %>% 
  mutate(text = sapply(text, stringi::stri_enc_toutf8, is_unknown_8bit=TRUE, validate=T)) %>% 
  unnest_tokens(sentence, text, token='sentences')

# get baby doc, convert to words
baby = barth %>% 
  filter(id=='baby.txt') %>% 
  mutate(sentence_id = 1:n()) %>% 
  unnest_tokens(word, sentence, drop=F) %>% 
  ungroup 

# get sentiment via inner join
baby_sentiment = baby %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by( sentence_id, sentence) %>% 
  summarise(score=sum(score)) %>% 
  ungroup

# plot sentiment over
baby_sentiment %>%
  plot_ly() %>% 
  add_paths(x=~sentence_id, y=~score,
            color=I('#00aaff')) %>%
  add_markers(x=~sentence_id, y=~score, 
              color=I('#ff5500'),
              size=I(10),
              hoverinfo=~ 'text', 
              text=~str_wrap(sentence),
              showlegend=F) %>% 
  theme_plotly()
```

inner join

return all rows from x where there are matching values in y, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned.


### Exercises

### Romeo & Juliet

<!--chapter:end:02_sentiment.Rmd-->

# POS tagging

As an initial review of parts of speech, I suggest you watch the following Schoolhouse Rocks videos:

- [A noun is a person, place, or thing.](https://youtu.be/h0m89e9oZko)
- [Interjections](https://youtu.be/YkAX7Vk3JEw)
- [Pronouns](https://youtu.be/Eu1ciVFbecw)
- [Verbs](https://youtu.be/US8mGU1MzYw)
- [Unpack your adjectives](https://youtu.be/NkuuZEey_bs)
- [Lolly Lolly Lolly Get Your Adverbs Here](https://youtu.be/14fXm4FOMPM)
- [Conjunction Junction](https://youtu.be/RPoBE-E8VOc) (personal fave)

Aside from those, you can also learn how bills get passed, about being a victim of gravity, a comparison of the decimal to other numeric systems, and a host of other useful things.

## Basic idea

With <span class="emph">part-of-speech</span> tagging, we classify a word with its corresponding part of speech. The following provides an example.

```{r pos_example, echo=FALSE}
rbind(c('JJ', 'JJ', 'NNS', 'VBP', 'RB'),
      c('Colorless', 'green', 'ideas', 'sleep', 'furiously.')) %>% 
  pander(justify='center')
       

```

We have two adjectives (JJ), a plural noun (NNS), a verb (VBP), and an adverb (RB).

Common analysis may then be used to predict POS given the current state of text, comparing the grammar of different texts, human-computer interaction, or translation from one language to another.

## Example

The following approach to POS-tagging is very similar to what we did for sentiment analysis as depicted previously. We have a POS dictionary, and can use an inner join to attach the words to their POS.  Unfortunately this approach is course unrealistically simplistic, as additional steps would need to be taken to ensure words are correctly classified.  Without more information, we are unable to tell if some words are being used, e.g. as nouns or verbs.  However, it serves as a starting point.

### Barthelme & Carver

In the following we'll compare three texts from Donald Barthelme:

- *The Balloon*
- *The First Thing The Baby Did Wrong*
- *Some Of Us Had Been Threatening Our Friend Colby*

As another comparison, I've included Raymond Carver's *What we talk about when we talk about love*, the unedited version.

```{r barthelme_pos}
balloon = barth %>% 
  filter(id=='balloon.txt') %>% 
  mutate(sentence_id = 1:n()) %>% 
  unnest_tokens(word, sentence, drop=F) %>% 
  ungroup 
colby = barth %>% 
  filter(id=='colby.txt') %>% 
  mutate(sentence_id = 1:n()) %>% 
  unnest_tokens(word, sentence, drop=F) %>% 
  ungroup 
barthelme_pos = balloon %>% 
  inner_join(parts_of_speech) %>% 
  count(pos) %>%
  mutate(text='balloon',
         prop=n/sum(n)) %>% 
  rbind(
    colby %>% 
      inner_join(parts_of_speech) %>% 
      count(pos) %>%
      mutate(text='colby',
             prop=n/sum(n)),
    baby %>% 
      inner_join(parts_of_speech) %>% 
      count(pos) %>%
      mutate(text='baby',
             prop=n/sum(n))
  )

```

```{r carver}
carver_pos = 
  data_frame(file = dir('data/texts_raw/carver/', full.names = TRUE)) %>%
  mutate(text = map(file, read_lines)) %>%
  transmute(id = basename(file), text) %>%
  unnest(text) %>% 
  unnest_tokens(word, text, token='words') %>% 
  inner_join(parts_of_speech) %>% 
  count(pos) %>%
  mutate(text='baby',
         prop=n/sum(n))
```

```{r barthelme_pos_vis}
carver_pos %>% 
  group_by(text) %>% 
  plot_ly() %>% 
  add_markers(x=~pos, y=~prop, color=I('gray50'), opacity=.5, size=I(20), name='Carver') %>% 
  add_markers(x=~pos, y=~prop, color=~text, size=I(10), data=barthelme_pos) %>% 
  theme_plotly()
```

It would appear Barthelme is fairly consistent. It would appear Carver preferred nouns and pronouns. 

### More taggin

More sophisticated POS tagging would require the context of the sentence structure. Luckily there are tools to help with that here, in particular via the <span class="pack">openNLP</span>.  In addition, it will require a certain language model to be installed (English is only one of many). I don't recommend doing so unless you are really interested in this (the <span class="pack">openNLPmodels.en</span> is fairly large).

This example more or less follows the help file example for `?Maxent_POS_Tag_Annotator`

```{r eval=FALSE, echo=FALSE}
# POS tagging in R with koRpus; requires installation of treeTagger
# activate library
library(koRpus)

# perform POS tagging
text.tagged <- treetag("data/texts_raw/carver/beginners.txt", 
                       treetagger="manual", 
                       lang="en",
                       TT.options=list(path="C:\\TreeTagger", preset="en"))
```
```{r, eval=F}
# install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")

library(NLP)
library(tm)  # make sure to load this prior to openNLP
library(openNLP)
library(openNLPmodels.en)
```


```{r baby_pos, eval=F}
load('data/barthelme_start.RData')

baby_string0 = barth0 %>% 
  filter(id=='baby.txt')

baby_string = unlist(baby_string0$text) %>% 
  paste(collapse=' ') %>% 
  as.String

init_s_w = annotate(baby_string, list(Maxent_Sent_Token_Annotator(),
                                      Maxent_Word_Token_Annotator()))
pos_res = annotate(baby_string, Maxent_POS_Tag_Annotator(), init_s_w)
word_subset = subset(pos_res, type=='word')
tags = sapply(word_subset$features , '[[', "POS")

baby_pos = data_frame(word=baby_string[word_subset], pos=tags) %>% 
  filter(!str_detect(pos, pattern='[[:punct:]]'))
```

```{r other_pos, eval=FALSE, echo=FALSE}
colby_string0 = barth0 %>% 
  filter(id=='colby.txt')

colby_string = unlist(colby_string0$text) %>% 
  paste(collapse=' ') %>% 
  as.String

init_s_w = annotate(colby_string, list(Maxent_Sent_Token_Annotator(),
                                      Maxent_Word_Token_Annotator()))
pos_res = annotate(colby_string, Maxent_POS_Tag_Annotator(), init_s_w)
word_subset = subset(pos_res, type=='word')
tags = sapply(word_subset$features , '[[', "POS")

colby_pos = data_frame(word=colby_string[word_subset], pos=tags) %>% 
  filter(!str_detect(pos, pattern='[[:punct:]]')) %>% 
  mutate(text='colby')


balloon_string0 = barth0 %>% 
  filter(id=='balloon.txt')

balloon_string = unlist(balloon_string0$text) %>% 
  paste(collapse=' ') %>% 
  as.String

init_s_w = annotate(balloon_string, list(Maxent_Sent_Token_Annotator(),
                                      Maxent_Word_Token_Annotator()))
pos_res = annotate(balloon_string, Maxent_POS_Tag_Annotator(), init_s_w)
word_subset = subset(pos_res, type=='word')
tags = sapply(word_subset$features , '[[', "POS")

balloon_pos = data_frame(word=balloon_string[word_subset], pos=tags) %>% 
  filter(!str_detect(pos, pattern='[[:punct:]]')) %>% 
  mutate(text='balloon')

barthelme_pos = baby_pos %>% 
  mutate(text='baby') %>% 
  bind_rows(colby_pos, balloon_pos) %>% 
  filter(pos != '``') %>% 
  data.frame  # because pander/dplyr issue
save(barthelme_pos, file='data/POS_results.RData')
```


Let's take a look. I've also done the other Barthelme texts as well for comparison.

```{r examine_baby_pos, echo=F}
load('data/POS_results.RData')
pander(barthelme_pos %>% head(20))
```

As we can see, we have quite a few more POS to deal with here.  They come from the [Penn Treebank](https://en.wikipedia.org/wiki/Treebank). The following table notes what the acronyms stand for. I don't pretend to know all the facets to this.

<img src="img/POS-Tags.png" style="display:block; margin: 0 auto;">

Ploting the differences, we now see a little more distinction between *The Balloon* and the other two texts. It is more likely to use the determiners, adjectives, singular nouns, and less likely to use personal pronouns and base verbs and verbs in past tense.

```{r barth_pos, eval=T, echo=F, cache=FALSE}
balloon_subset = barthelme_pos %>% 
  group_by(text) %>% 
  count(pos) %>%
  mutate(prop = n/sum(n)) %>% 
  filter(text=='balloon', pos %in% c('DT', 'JJ', 'NN', 'PRP', 'VB', 'VBD'))
barthelme_pos %>%
  group_by(text) %>%
  count(pos) %>%
  mutate(prop = n/sum(n)) %>%
  plot_ly(width=800) %>%
  add_markers(x=~pos, y=~prop, color=~text) %>%
  add_markers(x=~pos, y=~prop, color=~text, size=I(15),
              opacity=.5, data=balloon_subset, showlegend=F) %>%
  theme_plotly() %>%
  layout(xaxis = list(showgrid=T, 
                      gridcolor='#0000000D', 
                      title=F))
```

For more information, consult the following:

- [Penn Treebank](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports)
- [Maxent function](http://maxent.sourceforge.net/about.html)


For more natural language processing tools in R, consult the corresponding [task view](https://www.r-pkg.org/ctv/NaturalLanguageProcessing).  Much natural language processing is done with deep learning techniques, which generally requires a lot of data, notable computing resources, fine tunining, and often involves optimization towards a specific task.  Most of the cutting edge work there is done in Python, but as a starting point for more common approaches, you can check out the [Natural Language Toolkit](http://www.nltk.org/book/).

<!--chapter:end:03_pos.Rmd-->

# Topic modeling


## Basic idea

## Example

<!--chapter:end:04_topic.Rmd-->

